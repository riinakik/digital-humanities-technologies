{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUQcnFu7m03aml938sB6wN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riinakik/digital-humanities-technologies/blob/main/assignment_stanza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "For this assignment, I selected a short biographical text about the French composer Claude Debussy. The goal of the analysis was to explore the linguistic structure of the text using the Stanza natural language processing toolkit. Specifically, I examined how the text is organized at different linguistic levels: sentence structure, tokenization, parts of speech, lemmas, dependency relations, and named entities.\n",
        "I wanted to understand the text’s writing style, its distribution of parts of speech, and the proportions of nouns, verbs, and other grammatical categories. In addition, I analyzed the morphological and syntactic patterns to see how biographical information is presented through language."
      ],
      "metadata": {
        "id": "EcmpAaGMY-3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Change directory to your assignment folder\n",
        "%cd \"/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy\"\n",
        "\n",
        "# 3. Read the text file\n",
        "content = open(\"debussy.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# 4. Display the file to verify\n",
        "content\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "OXhPcUXk7pr2",
        "outputId": "901478e9-6b22-4dee-9b97-deb436b127b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Achille Claude Debussy (22 August 1862 – 25 March 1918) was a French composer. He is sometimes seen as the first Impressionist composer, although he rejected the term. He was one of the most influential composers of the late nineteenth and early twentieth centuries.\\n\\nBorn to a family of modest means and little cultural involvement, Debussy showed enough musical talent to be admitted at the age of ten to France's leading music college, the Conservatoire de Paris. He originally studied the piano, but found his vocation in innovative composition, despite the disapproval of the Conservatoire's conservative professors. He took many years to develop his mature style and was nearly forty when he achieved international fame in 1902 with the only opera he completed, Pelleas et Melisande.\\n\\nDebussy's orchestral works include Prelude to the Afternoon of a Faun (1894), Nocturnes (1897–1899), and Images (1905–1912). His music was in many ways a reaction against Wagner and the German musical tradition. He regarded the classical symphony as obsolete and sought an alternative in his symphonic sketches, The Sea (La mer), written between 1903 and 1905. His piano works include sets of twenty-four Preludes and twelve Etudes. Throughout his career, he wrote songs based on a wide variety of poetry, including his own. He was strongly influenced by the Symbolist poetic movement of the later nineteenth century.\\n\\nA number of his works, including the early The Blessed Damozel and the late The Martyrdom of Saint Sebastian, include significant parts for chorus. In his final years, he turned increasingly toward chamber music, completing three of six planned sonatas for different combinations of instruments.\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "Opens and reads the content of the debussy.txt file"
      ],
      "metadata": {
        "id": "1dN5gss4d4Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Stanza NLP library.\n",
        "!pip install stanza\n",
        "\n",
        "# Import the Stanza module and download the English language models.\n",
        "import stanza\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the Stanza NLP pipeline for English.\n",
        "nlp = stanza.Pipeline(\"en\")\n",
        "\n",
        "# Load the selected text file (\"debussy.txt\") into a Python string.\n",
        "# UTF-8 encoding ensures that special characters are handled correctly.\n",
        "content = open(\"debussy.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# Process the text using the Stanza pipeline.\n",
        "# This creates a 'doc' object that contains sentences, words, POS tags, lemmas, and more.\n",
        "doc = nlp(content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "03f9f9b7b79d444492d97ae2d10117b3",
            "af29b60cb4524a54a0f2cd304c105e9d",
            "933e0feda338472fa2b2ad341d915577",
            "891682656f3a4e58b552459491c51433",
            "22e521124435483d8ed539b136e17f5f",
            "9aa6c8286bdb41b0b9332bea7f166cb3",
            "de4f5d1dc1554d758b0e5f9401af3fa6",
            "cba736a627e54ed192a19875038cccb5",
            "a8e08367b305465da5c095590f46d69c",
            "7fbf98e9839b4a8984e505fbff02f068",
            "ba055eca409443088e88f0342d1a836b",
            "24384efb183043afb24d223700c62e81",
            "2af1486103a04df79c0fb12de41c4ef9",
            "3e5f6980e5494708b7032dd43a66eba4",
            "8d0729c81c544b88bc9bacdac9e73561",
            "c04720e3147743edb6c1a813330ee6f2",
            "6ac77cefaedb41d999d2581df7602b9c",
            "9fc1228435224b6fbe036e032b233b71",
            "ca80b598583342d390e9817cb61af029",
            "20bb9243fbb341fb89da6a5a44c7b0ff",
            "94bf293c13b542a9b89a2ed9a5d6bfe9",
            "168795e7b8044280a16183eb999dc961",
            "f9191df4c1fd484bac250691a409796f",
            "3083656cf91e42e6bb7b18cc5dead630",
            "bb828e6cc32a41ceb91f8019144232f0",
            "d07902e3a65d4020a51fffa39fc5d3f2",
            "4756517877f744fb812b111d59458a37",
            "d67f988d59544662a9316b941e3dd827",
            "5789d3fbda3d40b7b381e202c338dce0",
            "5a3b8e7c252b49cd8ac00985c5cd0ea3",
            "eabd2489eb284d22a6e7efbcb98eaea4",
            "7626f053eeda46498760c35c3500f1ef",
            "22ed9620e8e840c9a89af16c724d2f61"
          ]
        },
        "id": "899h_xxr9oPM",
        "outputId": "756e7e4c-d00f-4f3e-c53a-82ed9ba752fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stanza\n",
            "  Downloading stanza-1.11.0-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting emoji (from stanza)\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from stanza) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=3.15.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (5.29.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from stanza) (2.32.4)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from stanza) (3.6)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from stanza) (2.9.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stanza) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.14.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->stanza) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->stanza) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->stanza) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->stanza) (3.0.3)\n",
            "Downloading stanza-1.11.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji, stanza\n",
            "Successfully installed emoji-2.15.0 stanza-1.11.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "03f9f9b7b79d444492d97ae2d10117b3"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Downloading default packages for language: en (English) ...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.11.0/models/default.zip:   0%|          | …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "24384efb183043afb24d223700c62e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/en/default.zip\n",
            "INFO:stanza:Finished downloading models and saved to /root/stanza_resources\n",
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9191df4c1fd484bac250691a409796f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "============================================\n",
            "| Processor    | Package                   |\n",
            "--------------------------------------------\n",
            "| tokenize     | combined                  |\n",
            "| mwt          | combined                  |\n",
            "| pos          | combined_charlm           |\n",
            "| lemma        | combined_nocharlm         |\n",
            "| constituency | ptb3-revised_charlm       |\n",
            "| depparse     | combined_charlm           |\n",
            "| sentiment    | sstplus_charlm            |\n",
            "| ner          | ontonotes-ww-multi_charlm |\n",
            "============================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: constituency\n",
            "INFO:stanza:Loading: depparse\n",
            "INFO:stanza:Loading: sentiment\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "Stanza was successfully installed, and the English language model was downloaded.\n",
        "The text file debussy.txt was loaded, and the NLP pipeline processed it into a structured doc object containing sentences, tokens, lemmas, POS tags, and dependency information."
      ],
      "metadata": {
        "id": "9fgNW-xckEuX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of sentences in the processed document.\n",
        "len(doc.sentences)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BpLQBBk499zf",
        "outputId": "79105bf9-531e-4480-dfd1-b43ee66056e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "14"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "There are 14 sentences in this text"
      ],
      "metadata": {
        "id": "h_JX0h_oX_mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the full text of the first sentence.\n",
        "doc.sentences[0].text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5LLYD6PWDfVT",
        "outputId": "d282d695-58ae-4ce5-f169-7f8c7d3d7af4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Achille Claude Debussy (22 August 1862 – 25 March 1918) was a French composer.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "Shows the first sentence of the text: \"Achille Claude Debussy (22 August 1862 – 25 March 1918) was a French composer.\""
      ],
      "metadata": {
        "id": "IN-LOlmyerWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print all individual word tokens from the first sentence.\n",
        "[w.text for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xGJfeRqRDfie",
        "outputId": "5522512c-5d69-4f43-f246-3ba7ed464e16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Achille',\n",
              " 'Claude',\n",
              " 'Debussy',\n",
              " '(',\n",
              " '22',\n",
              " 'August',\n",
              " '1862',\n",
              " '–',\n",
              " '25',\n",
              " 'March',\n",
              " '1918',\n",
              " ')',\n",
              " 'was',\n",
              " 'a',\n",
              " 'French',\n",
              " 'composer',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows all individual tokens (words and symbols) from the first sentence of the text."
      ],
      "metadata": {
        "id": "vmFDLBinIZ0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all word tokens from the entire text, excluding punctuation.\n",
        "# This list collects every meaningful word Stanza identifies in all sentences.\n",
        "# We remove punctuation because it does not contribute to linguistic analysis.\n",
        "words = [w.text for s in doc.sentences for w in s.words if w.upos != \"PUNCT\"]\n",
        "\n",
        "# Display the total number of non-punctuation words in the text.\n",
        "len(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jfnM2zk9_Zvr",
        "outputId": "2bae22ec-0ab8-4c0c-ca58-dfe7d86e4225"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "276"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the total number (276) of meaningful words in the text after removing punctuation.\n"
      ],
      "metadata": {
        "id": "DfuywLOjI1EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all nouns from the entire text.\n",
        "# Nouns are identified by POS tags starting with \"NN\" (e.g., NN, NNS, NNP, NNPS).\n",
        "nouns = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"NN\")]\n",
        "\n",
        "# Extract all verbs from the entire text.\n",
        "# Verbs are identified by POS tags starting with \"VB\" (e.g., VB, VBD, VBG, VBN, VBP, VBZ).\n",
        "verbs = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"VB\")]\n",
        "\n",
        "# Display the total number of nouns and verbs.\n",
        "len(nouns), len(verbs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9PmhkzRm_bZ0",
        "outputId": "7b91bda9-71ee-4e85-cbcb-5d73476fea42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80, 33)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "A high noun count (80) vs verbs (33) shows that the text is descriptive and informational, which is typical of a biography.\n",
        "A lower verb count confirms that the text focuses on facts, descriptions, dates, and names, rather than actions or events."
      ],
      "metadata": {
        "id": "xi06-YMSNVlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of verbs relative to all meaningful words in the text.\n",
        "# This shows how verb-heavy the text is (amount of action or events described).\n",
        "len(verbs) / len(words)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAu1Mjxt_dTV",
        "outputId": "c3f6ae42-79e3-4488-9e88-72f09df8fb1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.11956521739130435"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This means that approximately 11.96% of all meaningful words in the text are verbs."
      ],
      "metadata": {
        "id": "9Kvp4gDHOM30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary that groups POS categories and their corresponding tag codes.\n",
        "# This allows us to count how many times each part of speech appears in the text.\n",
        "pos_tags = {\n",
        "    \"Conjunction\": [\"CC\"],\n",
        "    \"Pronoun\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
        "    \"Noun\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
        "    \"Verb\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
        "    \"Adjective\": [\"JJ\", \"JJR\", \"JJS\"]\n",
        "}\n",
        "\n",
        "# Count how many words belong to each POS category defined above.\n",
        "# The result is stored in a dictionary where each key is a POS name\n",
        "# and each value is the total count of that category in the text.\n",
        "results = {}\n",
        "for pos_name, tags in pos_tags.items():\n",
        "    results[pos_name] = len([w.text for s in doc.sentences for w in s.words if w.xpos in tags])\n",
        "\n",
        "# Display the POS frequency counts.\n",
        "results\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sw-VAGL_ivA",
        "outputId": "319cc01d-5389-448e-9adb-79b6a901fa47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Conjunction': 10, 'Pronoun': 20, 'Noun': 80, 'Verb': 33, 'Adjective': 36}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The POS distribution shows that the text is descriptive, fact-based, and biography-style, with a strong focus on nouns and adjectives rather than actions."
      ],
      "metadata": {
        "id": "E2VnwS2oO5En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the POS results by their frequency in descending order.\n",
        "# This makes it easy to see which parts of speech appear most often in the text.\n",
        "sorted(results.items(), key=lambda x: x[1], reverse=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s-GXPiEb_o6s",
        "outputId": "478e792e-2a41-4b16-f28e-b6b823652c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Noun', 80),\n",
              " ('Adjective', 36),\n",
              " ('Verb', 33),\n",
              " ('Pronoun', 20),\n",
              " ('Conjunction', 10)]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The distribution is noun-heavy, confirming that the text focuses on presenting information rather than narrating events or dialogue."
      ],
      "metadata": {
        "id": "FGpIiNnGXMyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the lemma (dictionary form) of every word in the text.\n",
        "lemmas = [w.lemma for s in doc.sentences for w in s.words]\n",
        "\n",
        "# Display the first 40 lemmas.\n",
        "lemmas[:40]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpJ-cqWnAyKh",
        "outputId": "5f2964e8-9af1-4532-ae95-33b247c05d3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Achille',\n",
              " 'Claude',\n",
              " 'Debussy',\n",
              " '(',\n",
              " '22',\n",
              " 'August',\n",
              " '1862',\n",
              " '-',\n",
              " '25',\n",
              " 'March',\n",
              " '1918',\n",
              " ')',\n",
              " 'be',\n",
              " 'a',\n",
              " 'French',\n",
              " 'composer',\n",
              " '.',\n",
              " 'he',\n",
              " 'be',\n",
              " 'sometimes',\n",
              " 'see',\n",
              " 'as',\n",
              " 'the',\n",
              " 'first',\n",
              " 'impressionist',\n",
              " 'composer',\n",
              " ',',\n",
              " 'although',\n",
              " 'he',\n",
              " 'reject',\n",
              " 'the',\n",
              " 'term',\n",
              " '.',\n",
              " 'he',\n",
              " 'be',\n",
              " 'one',\n",
              " 'of',\n",
              " 'the',\n",
              " 'most',\n",
              " 'influential']"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the first 40 lemmas of the text.\n",
        "\n",
        "A lemma is the base or dictionary form of a word. For example:\n",
        "\"was\" → \"be\"\n",
        "\"composers\" → \"composer\""
      ],
      "metadata": {
        "id": "0QWZrIhHTfg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of tuples showing the dependency relations in the first sentence.\n",
        "# For each word, we extract:\n",
        "# 1) the word form (w.text)\n",
        "# 2) the dependency label (w.deprel), showing the grammatical function\n",
        "# 3) the head word it depends on (syntactic governor)\n",
        "#\n",
        "# If w.head == 0, the word is the ROOT of the sentence.\n",
        "# Otherwise, w.head-1 gives the index of its governing word.\n",
        "[(w.text, w.deprel, doc.sentences[0].words[w.head-1].text if w.head > 0 else \"ROOT\")\n",
        " for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TUzyvakUA4-u",
        "outputId": "1ce82df0-8ea7-428b-aca1-c7342eb0c17d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Achille', 'nsubj', 'composer'),\n",
              " ('Claude', 'flat', 'Achille'),\n",
              " ('Debussy', 'flat', 'Achille'),\n",
              " ('(', 'punct', 'August'),\n",
              " ('22', 'nmod:unmarked', 'Achille'),\n",
              " ('August', 'compound', '22'),\n",
              " ('1862', 'nummod', 'August'),\n",
              " ('–', 'case', '25'),\n",
              " ('25', 'nmod', 'August'),\n",
              " ('March', 'compound', '25'),\n",
              " ('1918', 'nmod:unmarked', '25'),\n",
              " (')', 'punct', 'August'),\n",
              " ('was', 'cop', 'composer'),\n",
              " ('a', 'det', 'composer'),\n",
              " ('French', 'amod', 'composer'),\n",
              " ('composer', 'root', 'ROOT'),\n",
              " ('.', 'punct', 'composer')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the dependency relations for every word in the first sentence.\n",
        "\n",
        "For example:\n",
        "\n",
        "('Achille', 'nsubj', 'composer')\n",
        "→ Achille is the subject of the verb phrase headed by composer.\n",
        "\n",
        "('Claude', 'flat', 'Achille')\n",
        "→ Claude is linked to Achille as part of a name construction."
      ],
      "metadata": {
        "id": "vcQTcwe6T6uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Stanza pipeline with specific processors enabled:\n",
        "# - tokenize: split text into words\n",
        "# - pos: assign part-of-speech tags\n",
        "# - lemma: reduce words to their base form\n",
        "# - depparse: analyze syntactic dependency structure\n",
        "# - ner: identify named entities (people, locations, dates, organizations, etc.)\n",
        "nlp = stanza.Pipeline(\"en\", processors=\"tokenize,pos,lemma,depparse,ner\")\n",
        "\n",
        "# Process the text using the full pipeline.\n",
        "doc = nlp(content)\n",
        "\n",
        "# Extract all named entities from the text.\n",
        "# For each entity, we store:\n",
        "# 1) the entity text (ent.text)\n",
        "# 2) the entity type label (ent.type), such as PERSON, DATE, ORG, GPE (location), WORK_OF_ART, etc.\n",
        "[(ent.text, ent.type) for ent in doc.ents]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "bda6c6ddfe264ba5b8f5bdf629e4fc75",
            "befbc37973d642969c2409059d1e5b3f",
            "d1f0b23e54ee4e79bb19c9c6ef5b59a7",
            "dbb41bcc2fdf40a19e24a43fdbcb5cfd",
            "048ea9a33bba4d80a348c8d288143d3a",
            "6a548a8b7cf84d8ebd2655c88153b7c8",
            "0646dcfed3114aefa837441391305b75",
            "4145798ba76140cab22f6c5cabd07318",
            "a867e75013f342cbbb33b99e81c8a240",
            "15e1a72d5cfb4397b85d16aeea2deef7",
            "9a43e7b7ad6e4ebba1d7ebceba1aecab"
          ]
        },
        "id": "Yn4UGXIPA9AO",
        "outputId": "82f01b07-0f81-46f2-d3c9-3ced266d4a2a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.11.0.json:   0%|  …"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bda6c6ddfe264ba5b8f5bdf629e4fc75"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:stanza:Downloaded file to /root/stanza_resources/resources.json\n",
            "WARNING:stanza:Language en package default expects mwt, which has been added\n",
            "INFO:stanza:Loading these models for language: en (English):\n",
            "=========================================\n",
            "| Processor | Package                   |\n",
            "-----------------------------------------\n",
            "| tokenize  | combined                  |\n",
            "| mwt       | combined                  |\n",
            "| pos       | combined_charlm           |\n",
            "| lemma     | combined_nocharlm         |\n",
            "| depparse  | combined_charlm           |\n",
            "| ner       | ontonotes-ww-multi_charlm |\n",
            "=========================================\n",
            "\n",
            "INFO:stanza:Using device: cpu\n",
            "INFO:stanza:Loading: tokenize\n",
            "INFO:stanza:Loading: mwt\n",
            "INFO:stanza:Loading: pos\n",
            "INFO:stanza:Loading: lemma\n",
            "INFO:stanza:Loading: depparse\n",
            "INFO:stanza:Loading: ner\n",
            "INFO:stanza:Done loading processors!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Achille Claude Debussy', 'PERSON'),\n",
              " ('22 August 1862', 'DATE'),\n",
              " ('25 March 1918', 'DATE'),\n",
              " ('French', 'NORP'),\n",
              " ('first', 'ORDINAL'),\n",
              " ('one', 'CARDINAL'),\n",
              " ('the late nineteenth and early twentieth centuries', 'DATE'),\n",
              " ('Debussy', 'PERSON'),\n",
              " ('ten', 'CARDINAL'),\n",
              " (\"France's\", 'PERSON'),\n",
              " ('the Conservatoire de Paris', 'ORG'),\n",
              " (\"Conservatoire's\", 'NORP'),\n",
              " ('many years', 'DATE'),\n",
              " ('1902', 'DATE'),\n",
              " ('Pelleas et Melisande', 'PERSON'),\n",
              " ('Prelude to the Afternoon of a Faun', 'WORK_OF_ART'),\n",
              " ('1894', 'DATE'),\n",
              " ('Nocturnes', 'WORK_OF_ART'),\n",
              " ('1897', 'DATE'),\n",
              " ('1899', 'DATE'),\n",
              " ('Images', 'WORK_OF_ART'),\n",
              " ('1905', 'DATE'),\n",
              " ('1912', 'DATE'),\n",
              " ('Wagner', 'PERSON'),\n",
              " ('German', 'NORP'),\n",
              " ('The Sea (La mer)', 'WORK_OF_ART'),\n",
              " ('1903', 'DATE'),\n",
              " ('1905', 'DATE'),\n",
              " ('twenty-four', 'CARDINAL'),\n",
              " ('twelve', 'CARDINAL'),\n",
              " ('Symbolist', 'NORP'),\n",
              " ('the later nineteenth century', 'DATE'),\n",
              " ('The Blessed Damozel', 'WORK_OF_ART'),\n",
              " ('The Martyrdom of Saint Sebastian', 'WORK_OF_ART'),\n",
              " ('his final years', 'DATE'),\n",
              " ('three', 'CARDINAL'),\n",
              " ('six', 'CARDINAL')]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "In this text, Stanza successfully identifies important biographical information about Claude Debussy.\n",
        "\n",
        "For example:\n",
        "“Achille Claude Debussy” → PERSON.\n",
        "“22 August 1862” → DATE.\n",
        "“the Conservatoire de Paris” → ORG."
      ],
      "metadata": {
        "id": "ntAc1_URUQ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For every word in the first sentence, extract three pieces of information:\n",
        "# 1) w.text  → the original word form in the sentence\n",
        "# 2) w.xpos  → the detailed POS tag (Penn Treebank / universal POS extension)\n",
        "# 3) w.feats → morphological features, such as Number, Tense, Person, Mood, Case, Gender, etc.\n",
        "\n",
        "# This gives a detailed grammatical profile of each word,\n",
        "# allowing deeper analysis of how the sentence is structured linguistically.\n",
        "[(w.text, w.xpos, w.feats) for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lGUptmKQBGeD",
        "outputId": "a77b78ce-cf54-4c17-8af3-b5d49f5b8baf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Achille', 'NNP', 'Number=Sing'),\n",
              " ('Claude', 'NNP', 'Number=Sing'),\n",
              " ('Debussy', 'NNP', 'Number=Sing'),\n",
              " ('(', '-LRB-', None),\n",
              " ('22', 'CD', 'NumForm=Digit|NumType=Card'),\n",
              " ('August', 'NNP', 'Number=Sing'),\n",
              " ('1862', 'CD', 'NumForm=Digit|NumType=Card'),\n",
              " ('–', 'SYM', None),\n",
              " ('25', 'CD', 'NumForm=Digit|NumType=Card'),\n",
              " ('March', 'NNP', 'Number=Sing'),\n",
              " ('1918', 'CD', 'NumForm=Digit|NumType=Card'),\n",
              " (')', '-RRB-', None),\n",
              " ('was', 'VBD', 'Mood=Ind|Number=Sing|Person=3|Tense=Past|VerbForm=Fin'),\n",
              " ('a', 'DT', 'Definite=Ind|PronType=Art'),\n",
              " ('French', 'JJ', 'Degree=Pos'),\n",
              " ('composer', 'NN', 'Number=Sing'),\n",
              " ('.', '.', None)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This output provides a deep grammatical breakdown of each token in the sentence.\n",
        "It shows:\n",
        "\n",
        "Names (e.g., Achille, Claude, Debussy) marked as NNP, Number=Sing.\n",
        "\n",
        "Dates and numbers correctly recognized as numerals (CD, NumForm=Digit).\n",
        "\n",
        "Verbs with rich grammatical detail (e.g., was → VBD, with features indicating past tense).\n",
        "\n",
        "Punctuation and symbols categorized appropriately.\n",
        "\n",
        "Nouns and adjectives tagged with their syntactic and morphological information."
      ],
      "metadata": {
        "id": "zA6jwvLFU8ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# ------------------------------\n",
        "# ANALYSIS PART\n",
        "# ------------------------------\n",
        "\n",
        "num_sentences = len(doc.sentences)\n",
        "words = [w.text for s in doc.sentences for w in s.words if w.upos != \"PUNCT\"]\n",
        "nouns = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"NN\")]\n",
        "verbs = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"VB\")]\n",
        "verb_ratio = len(verbs) / len(words)\n",
        "\n",
        "pos_tags = {\n",
        "    \"Conjunction\": [\"CC\"],\n",
        "    \"Pronoun\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
        "    \"Noun\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
        "    \"Verb\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
        "    \"Adjective\": [\"JJ\", \"JJR\", \"JJS\"]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for pos_name, tags in pos_tags.items():\n",
        "    results[pos_name] = len([w for s in doc.sentences for w in s.words if w.xpos in tags])\n",
        "\n",
        "# ------------------------------\n",
        "# 4. CREATE OUTPUT FOLDER\n",
        "# ------------------------------\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy/output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "output_file = os.path.join(output_folder, \"analysis.txt\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. WRITE ANALYSIS TO FILE\n",
        "# ------------------------------\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
        "    out.write(\"Analysis of the Selected Text\\n\")\n",
        "    out.write(\"------------------------------------\\n\\n\")\n",
        "\n",
        "    out.write(f\"Number of sentences: {num_sentences}\\n\")\n",
        "    out.write(f\"Total meaningful words (no punctuation): {len(words)}\\n\")\n",
        "    out.write(f\"Noun count: {len(nouns)}\\n\")\n",
        "    out.write(f\"Verb count: {len(verbs)}\\n\")\n",
        "    out.write(f\"Proportion of verbs: {verb_ratio:.2f}\\n\\n\")\n",
        "\n",
        "    out.write(\"Part-of-Speech distribution:\\n\")\n",
        "    for pos_name, count in results.items():\n",
        "        out.write(f\"  {pos_name}: {count}\\n\")\n",
        "\n",
        "    out.write(\"\\nNamed Entities:\\n\")\n",
        "    for ent in doc.ents:\n",
        "        out.write(f\"  {ent.text}  -->  {ent.type}\\n\")\n",
        "\n",
        "print(\"analysis.txt has been created successfully!\")\n",
        "print(\"Saved to:\", output_file)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KM9W-4SAZxAV",
        "outputId": "9b057db7-cd89-4069-c169-4ad558cf9abe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "analysis.txt has been created successfully!\n",
            "Saved to: /content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy/output/analysis.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This code generates a new folder and creates an output file named analysis.txt inside it.\n",
        "\n",
        "The file contains a summary of the linguistic features extracted from the Claude Debussy biography using the Stanza NLP pipeline."
      ],
      "metadata": {
        "id": "nsmb3f4nbhEY"
      }
    }
  ]
}