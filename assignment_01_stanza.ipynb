{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2nYIki0wg/XQ7weBA+qdq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riinakik/digital-humanities-technologies/blob/main/assignment_01_stanza.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Introduction**\n",
        "\n",
        "For this assignment, I selected a short biographical text about the French composer Claude Debussy. The goal of the analysis was to explore the linguistic structure of the text using the Stanza natural language processing toolkit. Specifically, I examined how the text is organized at different linguistic levels: sentence structure, tokenization, parts of speech, lemmas, dependency relations, and named entities.\n",
        "I wanted to understand the text’s writing style, its distribution of parts of speech, and the proportions of nouns, verbs, and other grammatical categories. In addition, I analyzed the morphological and syntactic patterns to see how biographical information is presented through language."
      ],
      "metadata": {
        "id": "EcmpAaGMY-3J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change directory to the assignment folder in Google Drive.\n",
        "%cd \"/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy\"\n",
        "\n",
        "# Open the text file containing the Claude Debussy biography.\n",
        "# \"r\" means reading mode and UTF-8 ensures correct handling of special characters.\n",
        "content = open(\"/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy/debussy.txt\",\n",
        "               \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# Display the loaded text to verify that the file was successfully read.\n",
        "content\n"
      ],
      "metadata": {
        "id": "OXhPcUXk7pr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "Opens and reads the content of the debussy.txt file"
      ],
      "metadata": {
        "id": "1dN5gss4d4Hn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the Stanza NLP library.\n",
        "!pip install stanza\n",
        "\n",
        "# Import the Stanza module and download the English language models.\n",
        "import stanza\n",
        "stanza.download(\"en\")\n",
        "\n",
        "# Initialize the Stanza NLP pipeline for English.\n",
        "nlp = stanza.Pipeline(\"en\")\n",
        "\n",
        "# Load the selected text file (\"debussy.txt\") into a Python string.\n",
        "# UTF-8 encoding ensures that special characters are handled correctly.\n",
        "content = open(\"debussy.txt\", \"r\", encoding=\"utf-8\").read()\n",
        "\n",
        "# Process the text using the Stanza pipeline.\n",
        "# This creates a 'doc' object that contains sentences, words, POS tags, lemmas, and more.\n",
        "doc = nlp(content)\n",
        "\n"
      ],
      "metadata": {
        "id": "899h_xxr9oPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Print the number of sentences in the processed document.\n",
        "len(doc.sentences)\n"
      ],
      "metadata": {
        "id": "BpLQBBk499zf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "There are 14 sentences in this text"
      ],
      "metadata": {
        "id": "h_JX0h_oX_mO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the full text of the first sentence.\n",
        "doc.sentences[0].text"
      ],
      "metadata": {
        "id": "5LLYD6PWDfVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "Shows the first sentence of the text: \"Achille Claude Debussy (22 August 1862 – 25 March 1918) was a French composer.\""
      ],
      "metadata": {
        "id": "IN-LOlmyerWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract and print all individual word tokens from the first sentence.\n",
        "[w.text for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "id": "xGJfeRqRDfie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows all individual tokens (words and symbols) from the first sentence of the text."
      ],
      "metadata": {
        "id": "vmFDLBinIZ0O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all word tokens from the entire text, excluding punctuation.\n",
        "# This list collects every meaningful word Stanza identifies in all sentences.\n",
        "# We remove punctuation because it does not contribute to linguistic analysis.\n",
        "words = [w.text for s in doc.sentences for w in s.words if w.upos != \"PUNCT\"]\n",
        "\n",
        "# Display the total number of non-punctuation words in the text.\n",
        "len(words)\n"
      ],
      "metadata": {
        "id": "jfnM2zk9_Zvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the total number (276) of meaningful words in the text after removing punctuation.\n"
      ],
      "metadata": {
        "id": "DfuywLOjI1EX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract all nouns from the entire text.\n",
        "# Nouns are identified by POS tags starting with \"NN\" (e.g., NN, NNS, NNP, NNPS).\n",
        "nouns = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"NN\")]\n",
        "\n",
        "# Extract all verbs from the entire text.\n",
        "# Verbs are identified by POS tags starting with \"VB\" (e.g., VB, VBD, VBG, VBN, VBP, VBZ).\n",
        "verbs = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"VB\")]\n",
        "\n",
        "# Display the total number of nouns and verbs.\n",
        "len(nouns), len(verbs)"
      ],
      "metadata": {
        "id": "9PmhkzRm_bZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "A high noun count (80) vs verbs (33) shows that the text is descriptive and informational, which is typical of a biography.\n",
        "A lower verb count confirms that the text focuses on facts, descriptions, dates, and names, rather than actions or events."
      ],
      "metadata": {
        "id": "xi06-YMSNVlP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate the proportion of verbs relative to all meaningful words in the text.\n",
        "# This shows how verb-heavy the text is (amount of action or events described).\n",
        "len(verbs) / len(words)\n"
      ],
      "metadata": {
        "id": "LAu1Mjxt_dTV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This means that approximately 11.96% of all meaningful words in the text are verbs."
      ],
      "metadata": {
        "id": "9Kvp4gDHOM30"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary that groups POS categories and their corresponding tag codes.\n",
        "# This allows us to count how many times each part of speech appears in the text.\n",
        "pos_tags = {\n",
        "    \"Conjunction\": [\"CC\"],\n",
        "    \"Pronoun\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
        "    \"Noun\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
        "    \"Verb\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
        "    \"Adjective\": [\"JJ\", \"JJR\", \"JJS\"]\n",
        "}\n",
        "\n",
        "# Count how many words belong to each POS category defined above.\n",
        "# The result is stored in a dictionary where each key is a POS name\n",
        "# and each value is the total count of that category in the text.\n",
        "results = {}\n",
        "for pos_name, tags in pos_tags.items():\n",
        "    results[pos_name] = len([w.text for s in doc.sentences for w in s.words if w.xpos in tags])\n",
        "\n",
        "# Display the POS frequency counts.\n",
        "results\n"
      ],
      "metadata": {
        "id": "8Sw-VAGL_ivA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The POS distribution shows that the text is descriptive, fact-based, and biography-style, with a strong focus on nouns and adjectives rather than actions."
      ],
      "metadata": {
        "id": "E2VnwS2oO5En"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sort the POS results by their frequency in descending order.\n",
        "# This makes it easy to see which parts of speech appear most often in the text.\n",
        "sorted(results.items(), key=lambda x: x[1], reverse=True)\n"
      ],
      "metadata": {
        "id": "s-GXPiEb_o6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The distribution is noun-heavy, confirming that the text focuses on presenting information rather than narrating events or dialogue."
      ],
      "metadata": {
        "id": "FGpIiNnGXMyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the lemma (dictionary form) of every word in the text.\n",
        "lemmas = [w.lemma for s in doc.sentences for w in s.words]\n",
        "\n",
        "# Display the first 40 lemmas.\n",
        "lemmas[:40]\n"
      ],
      "metadata": {
        "id": "wpJ-cqWnAyKh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the first 40 lemmas of the text.\n",
        "\n",
        "A lemma is the base or dictionary form of a word. For example:\n",
        "\"was\" → \"be\"\n",
        "\"composers\" → \"composer\""
      ],
      "metadata": {
        "id": "0QWZrIhHTfg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate a list of tuples showing the dependency relations in the first sentence.\n",
        "# For each word, we extract:\n",
        "# 1) the word form (w.text)\n",
        "# 2) the dependency label (w.deprel), showing the grammatical function\n",
        "# 3) the head word it depends on (syntactic governor)\n",
        "#\n",
        "# If w.head == 0, the word is the ROOT of the sentence.\n",
        "# Otherwise, w.head-1 gives the index of its governing word.\n",
        "[(w.text, w.deprel, doc.sentences[0].words[w.head-1].text if w.head > 0 else \"ROOT\")\n",
        " for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "id": "TUzyvakUA4-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "The output shows the dependency relations for every word in the first sentence.\n",
        "\n",
        "For example:\n",
        "\n",
        "('Achille', 'nsubj', 'composer')\n",
        "→ Achille is the subject of the verb phrase headed by composer.\n",
        "\n",
        "('Claude', 'flat', 'Achille')\n",
        "→ Claude is linked to Achille as part of a name construction."
      ],
      "metadata": {
        "id": "vcQTcwe6T6uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Stanza pipeline with specific processors enabled:\n",
        "# - tokenize: split text into words\n",
        "# - pos: assign part-of-speech tags\n",
        "# - lemma: reduce words to their base form\n",
        "# - depparse: analyze syntactic dependency structure\n",
        "# - ner: identify named entities (people, locations, dates, organizations, etc.)\n",
        "nlp = stanza.Pipeline(\"en\", processors=\"tokenize,pos,lemma,depparse,ner\")\n",
        "\n",
        "# Process the text using the full pipeline.\n",
        "doc = nlp(content)\n",
        "\n",
        "# Extract all named entities from the text.\n",
        "# For each entity, we store:\n",
        "# 1) the entity text (ent.text)\n",
        "# 2) the entity type label (ent.type), such as PERSON, DATE, ORG, GPE (location), WORK_OF_ART, etc.\n",
        "[(ent.text, ent.type) for ent in doc.ents]\n"
      ],
      "metadata": {
        "id": "Yn4UGXIPA9AO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "In this text, Stanza successfully identifies important biographical information about Claude Debussy.\n",
        "\n",
        "For example:\n",
        "“Achille Claude Debussy” → PERSON.\n",
        "“22 August 1862” → DATE.\n",
        "“the Conservatoire de Paris” → ORG."
      ],
      "metadata": {
        "id": "ntAc1_URUQ_O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For every word in the first sentence, extract three pieces of information:\n",
        "# 1) w.text  → the original word form in the sentence\n",
        "# 2) w.xpos  → the detailed POS tag (Penn Treebank / universal POS extension)\n",
        "# 3) w.feats → morphological features, such as Number, Tense, Person, Mood, Case, Gender, etc.\n",
        "\n",
        "# This gives a detailed grammatical profile of each word,\n",
        "# allowing deeper analysis of how the sentence is structured linguistically.\n",
        "[(w.text, w.xpos, w.feats) for w in doc.sentences[0].words]\n"
      ],
      "metadata": {
        "id": "lGUptmKQBGeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This output provides a deep grammatical breakdown of each token in the sentence.\n",
        "It shows:\n",
        "\n",
        "Names (e.g., Achille, Claude, Debussy) marked as NNP, Number=Sing.\n",
        "\n",
        "Dates and numbers correctly recognized as numerals (CD, NumForm=Digit).\n",
        "\n",
        "Verbs with rich grammatical detail (e.g., was → VBD, with features indicating past tense).\n",
        "\n",
        "Punctuation and symbols categorized appropriately.\n",
        "\n",
        "Nouns and adjectives tagged with their syntactic and morphological information."
      ],
      "metadata": {
        "id": "zA6jwvLFU8ix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# ANALYSIS PART\n",
        "# ------------------------------\n",
        "\n",
        "num_sentences = len(doc.sentences)\n",
        "words = [w.text for s in doc.sentences for w in s.words if w.upos != \"PUNCT\"]\n",
        "nouns = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"NN\")]\n",
        "verbs = [w.text for s in doc.sentences for w in s.words if w.xpos.startswith(\"VB\")]\n",
        "verb_ratio = len(verbs) / len(words)\n",
        "\n",
        "pos_tags = {\n",
        "    \"Conjunction\": [\"CC\"],\n",
        "    \"Pronoun\": [\"PRP\", \"PRP$\", \"WP\", \"WP$\"],\n",
        "    \"Noun\": [\"NN\", \"NNS\", \"NNP\", \"NNPS\"],\n",
        "    \"Verb\": [\"VB\", \"VBD\", \"VBG\", \"VBN\", \"VBP\", \"VBZ\"],\n",
        "    \"Adjective\": [\"JJ\", \"JJR\", \"JJS\"]\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for pos_name, tags in pos_tags.items():\n",
        "    results[pos_name] = len([w for s in doc.sentences for w in s.words if w.xpos in tags])\n",
        "\n",
        "# ------------------------------\n",
        "# 4. CREATE OUTPUT FOLDER\n",
        "# ------------------------------\n",
        "\n",
        "output_folder = \"/content/drive/MyDrive/Digihumanitaaria tehnoloogiad/assignment_debussy/output\"\n",
        "os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "output_file = os.path.join(output_folder, \"analysis.txt\")\n",
        "\n",
        "# ------------------------------\n",
        "# 5. WRITE ANALYSIS TO FILE\n",
        "# ------------------------------\n",
        "\n",
        "with open(output_file, \"w\", encoding=\"utf-8\") as out:\n",
        "    out.write(\"Analysis of the Selected Text\\n\")\n",
        "    out.write(\"------------------------------------\\n\\n\")\n",
        "\n",
        "    out.write(f\"Number of sentences: {num_sentences}\\n\")\n",
        "    out.write(f\"Total meaningful words (no punctuation): {len(words)}\\n\")\n",
        "    out.write(f\"Noun count: {len(nouns)}\\n\")\n",
        "    out.write(f\"Verb count: {len(verbs)}\\n\")\n",
        "    out.write(f\"Proportion of verbs: {verb_ratio:.2f}\\n\\n\")\n",
        "\n",
        "    out.write(\"Part-of-Speech distribution:\\n\")\n",
        "    for pos_name, count in results.items():\n",
        "        out.write(f\"  {pos_name}: {count}\\n\")\n",
        "\n",
        "    out.write(\"\\nNamed Entities:\\n\")\n",
        "    for ent in doc.ents:\n",
        "        out.write(f\"  {ent.text}  -->  {ent.type}\\n\")\n",
        "\n",
        "print(\"analysis.txt has been created successfully!\")\n",
        "print(\"Saved to:\", output_file)\n"
      ],
      "metadata": {
        "id": "KM9W-4SAZxAV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Result**\n",
        "\n",
        "This code generates a new folder and creates an output file named analysis.txt inside it.\n",
        "\n",
        "The file contains a summary of the linguistic features extracted from the Claude Debussy biography using the Stanza NLP pipeline."
      ],
      "metadata": {
        "id": "nsmb3f4nbhEY"
      }
    }
  ]
}